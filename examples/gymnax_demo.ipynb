{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqnWMjdLMbUI"
   },
   "source": [
    "# `gymnax`: Classic Gym Environments in JAX\n",
    "### [Last Update: June 2022][![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RobertTLange/gymnax/blob/main/examples/getting_started.ipynb)\n",
    "\n",
    "Welcome to `gymnax`, the one stop shop for fast classic Reinforcement Learning environments powered by JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scafRWftMbUX"
   },
   "source": [
    "## Basic API: `gymnax.make()`, `env.reset()`, `env.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "99Wuqa2lMbUY",
    "outputId": "cde91b85-1b79-4f98-a655-581bfd168477",
    "ExecuteTime": {
     "end_time": "2024-03-24T16:44:16.861309Z",
     "start_time": "2024-03-24T16:44:11.803004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1, [CpuDevice(id=0)])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the number of (emulated) host devices\n",
    "#num_devices = 8\n",
    "#os.environ['XLA_FLAGS'] = f\"--xla_force_host_platform_device_count={num_devices}\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import gymnax\n",
    "import networkx as nx\n",
    "import json\n",
    "import dataclasses\n",
    "from xlron.environments.env_funcs import *\n",
    "from xlron.environments.rsa import *\n",
    "from xlron.environments.vone import *\n",
    "\n",
    "num_devices = jax.device_count()\n",
    "\n",
    "jax.default_device = jax.devices()[0]\n",
    "\n",
    "jax.device_count(), jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_vone_env() got an unexpected keyword argument 'topology_name'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      2\u001B[0m rng, key_init, key_reset, key_policy, key_step \u001B[38;5;241m=\u001B[39m jax\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39msplit(rng, \u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#env = RSAEnv(key_init, graph, env_params)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#env = VONEEnv(key_init, graph, env_params)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#env, env_params = make_rsa_env()\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m env, env_params \u001B[38;5;241m=\u001B[39m \u001B[43mmake_vone_env\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtopology_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnsfnet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_node_resources\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_slots\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_slots\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Inspect default environment settings\u001B[39;00m\n\u001B[1;32m     11\u001B[0m env_params\n",
      "\u001B[0;31mTypeError\u001B[0m: make_vone_env() got an unexpected keyword argument 'topology_name'"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_init, key_reset, key_policy, key_step = jax.random.split(rng, 5)\n",
    "\n",
    "#env = RSAEnv(key_init, graph, env_params)\n",
    "\n",
    "#env = VONEEnv(key_init, graph, env_params)\n",
    "#env, env_params = make_rsa_env()\n",
    "env, env_params = make_vone_env(topology_name=\"nsfnet\", max_node_resources=2, min_slots=2, max_slots=4, load=60)\n",
    "\n",
    "# Inspect default environment settings\n",
    "env_params\n",
    "#dataclasses.fields(env_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T16:44:19.311318Z",
     "start_time": "2024-03-24T16:44:19.005371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saU94v-0MbUg",
    "outputId": "e317b207-9ded-41a6-cdc4-307f650d1fa7"
   },
   "outputs": [],
   "source": [
    "obs, state = env.reset(key_reset, env_params)\n",
    "#obs, state = env.reset(key_reset, env_params.path_link_array)\n",
    "obs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcygbthfMbUi",
    "outputId": "2afa634d-e3d5-4cfa-de91-abcea0bfc556"
   },
   "outputs": [],
   "source": [
    "action = env.action_space(env_params).sample(key_policy)\n",
    "#print(jnp.squeeze(action).shape)\n",
    "#print(action.shape)\n",
    "print(action)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "n_obs, n_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxvm0d_tMbUl",
    "ExecuteTime": {
     "end_time": "2023-06-07T17:59:06.037884Z",
     "start_time": "2023-06-07T17:59:05.934455Z"
    }
   },
   "source": [
    "`gymnax` provides fully functional environment dynamics that can leverage the full power of JAX's function transformations. E.g. one common RL use-case the parallel rollout of multiple workers. Using a `vmap` across random seeds (one per worker) allows us to implement such a parallelization on a single machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWuKmWuxMbUm",
    "outputId": "20d002f1-5e92-4bea-8f8f-026ee54aa634"
   },
   "outputs": [],
   "source": [
    "vmap_reset = jax.vmap(env.reset, in_axes=(0, None))\n",
    "vmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "num_envs = 10\n",
    "vmap_keys = jax.random.split(rng, num_envs)\n",
    "\n",
    "obs, state = vmap_reset(vmap_keys, env_params)\n",
    "if isinstance(env, VONEEnv):\n",
    "    n_obs, n_state, reward, done, _ = vmap_step(vmap_keys, state, (jnp.zeros(num_envs), jnp.zeros(num_envs), jnp.zeros(num_envs)), env_params)\n",
    "elif isinstance(env, RSAEnv):\n",
    "    n_obs, n_state, reward, done, _ = vmap_step(vmap_keys, state, jnp.zeros(num_envs), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9_4d5BDMbUn"
   },
   "source": [
    "Similarly, you can also choose to `pmap` across rollout workers (\"actors\") across multiple devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4TUmdcYMbUp",
    "outputId": "c8adf2ee-157e-41d0-a6ca-20166c5773fd"
   },
   "outputs": [],
   "source": [
    "pmap_reset = jax.pmap(env.reset, in_axes=(0, None), static_broadcasted_argnums=(1))\n",
    "pmap_step = jax.pmap(env.step, in_axes=(0, 0, 0, None), static_broadcasted_argnums=(3))\n",
    "\n",
    "\n",
    "pmap_keys = jax.random.split(rng, num_devices)\n",
    "obs, state = pmap_reset(pmap_keys, env_params)\n",
    "if isinstance(env, VONEEnv):\n",
    "    n_obs, n_state, reward, done, _ = pmap_step(pmap_keys, state, (jnp.zeros(num_devices), jnp.zeros(num_devices), jnp.zeros(num_devices)), env_params)\n",
    "elif isinstance(env, RSAEnv):\n",
    "    n_obs, n_state, reward, done, _ = pmap_step(pmap_keys, state, jnp.zeros(num_devices), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zERpkJyxMbUr",
    "ExecuteTime": {
     "end_time": "2023-06-07T18:20:20.615644Z",
     "start_time": "2023-06-07T18:20:20.582925Z"
    }
   },
   "source": [
    "The code above has executed each worker-specific environment transition on a separate device, but we can also chain `vmap` and `pmap` to execute multiple workers on a single device and at the same time across multiple devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvjOrY2oMbUs",
    "outputId": "177df93c-c8b3-47f4-cb84-4c11df6ab2da"
   },
   "outputs": [],
   "source": [
    "map_reset = jax.pmap(vmap_reset, in_axes=(0, None), static_broadcasted_argnums=(1))\n",
    "map_step = jax.pmap(vmap_step, in_axes=(0, 0, 0, None), static_broadcasted_argnums=(3))\n",
    "\n",
    "map_keys = jnp.tile(vmap_keys, (num_devices, 1, 1))\n",
    "obs, state = map_reset(map_keys, env_params)\n",
    "if isinstance(env, VONEEnv):\n",
    "    n_obs, n_state, reward, done, _ = map_step(map_keys, state, (jnp.zeros((num_devices, num_envs)), jnp.zeros((num_devices, num_envs)), jnp.zeros((num_devices, num_envs))), env_params)\n",
    "elif isinstance(env, RSAEnv):\n",
    "    n_obs, n_state, reward, done, _ = map_step(map_keys, state, jnp.zeros((num_devices, num_envs)), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5N4Pup3MbUu"
   },
   "source": [
    "We can now easily leverage massive accelerator parallelism to churn through millions/billions of environment transitions when training 'sentient' agents. Note that in the code snippet above we have executed 4 times the same 8 environment workers, since we tiled/repeated the same key across the device axis. In general `pmap`-ing will require you to pay special attention to the shapes of the arrays that come out your operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFyvlS0PMbUw",
    "ExecuteTime": {
     "end_time": "2023-06-07T18:24:17.422371Z",
     "start_time": "2023-06-07T18:24:17.335158Z"
    }
   },
   "source": [
    "## Jitted Episode Rollouts via `lax.scan`\n",
    "\n",
    "Let's now walk through an example of using `gymnax` with one of the common neural network libraries to parametrize a simple policy: `flax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFH3vBzAMbUw"
   },
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple ReLU MLP.\"\"\"\n",
    "\n",
    "    num_hidden_units: int\n",
    "    num_hidden_layers: int\n",
    "    num_output_units: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, rng):\n",
    "        for l in range(self.num_hidden_layers):\n",
    "            x = nn.Dense(features=self.num_hidden_units)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.num_output_units)(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = MLP(64, 2, 3)\n",
    "policy_params = model.init(rng, jnp.zeros(2128), None)\n",
    "#policy_params = model.init(rng, jnp.zeros(319904), None)\n",
    "# obs = env.get_obs(state)\n",
    "# model_action = model.apply(policy_params, obs, key_step)\n",
    "# sample_action = env.action_space(env_params).sample(key_policy)\n",
    "# model_action, sample_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L__J4fM1MbUy"
   },
   "outputs": [],
   "source": [
    "def rollout(rng_input, policy_params, env_params, steps_in_episode):\n",
    "    \"\"\"Rollout a jitted gymnax episode with lax.scan.\"\"\"\n",
    "    # Reset the environment\n",
    "    rng_reset, rng_episode = jax.random.split(rng_input)\n",
    "    obs, state = env.reset(rng_reset, env_params)\n",
    "\n",
    "    def policy_step(state_input, tmp):\n",
    "        \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "        obs, state, policy_params, rng = state_input\n",
    "        rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "        action = jnp.squeeze(model.apply(policy_params, obs, rng_net)) # Squeeze works for RSAEnv!\n",
    "        next_obs, next_state, reward, done, _ = env.step(\n",
    "          rng_step, state, action, env_params\n",
    "        )\n",
    "        carry = [next_obs, next_state, policy_params, rng]\n",
    "        return carry, [obs, action, reward, next_obs, done]\n",
    "\n",
    "    # Scan over episode step loop\n",
    "    _, scan_out = jax.lax.scan(\n",
    "      policy_step,\n",
    "      [obs, state, policy_params, rng_episode],\n",
    "      (),\n",
    "      steps_in_episode\n",
    "    )\n",
    "    # Return masked sum of rewards accumulated by agent in episode\n",
    "    obs, action, reward, next_obs, done = scan_out\n",
    "    return obs, action, reward, next_obs, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-49pv-kvMbUz",
    "outputId": "ba06628a-bb2d-403a-f7e8-e6a8ac09e678"
   },
   "outputs": [],
   "source": [
    "# Jit-Compiled Episode Rollout\n",
    "jit_rollout = jax.jit(rollout, static_argnums=(2, 3,))\n",
    "obs, action, reward, next_obs, done = jit_rollout(rng, policy_params, env_params, 10000)\n",
    "obs.shape, reward.shape, jnp.sum(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iazv9WkCMbU0"
   },
   "source": [
    "Again, you can wrap this `rollout` function with the magic of JAX and for all implemented RL environments. But we also provide a simple that does so for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTu75LFWMbU0"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import gymnax\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "from gymnax.environments import environment\n",
    "import timeit\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "\n",
    "# TODO: Add RNN forward with init_carry/hidden\n",
    "# TODO: Add pmap utitlities if multi-device\n",
    "# TODO: Use as backend in `GymFitness` or keep separated?\n",
    "\n",
    "\n",
    "class RolloutWrapper(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_forward=None,\n",
    "        env: environment.Environment = None,\n",
    "        num_env_steps: Optional[int] = None,\n",
    "        env_params: EnvParams = None,\n",
    "    ):\n",
    "        \"\"\"Wrapper to define batch evaluation for generation parameters.\"\"\"\n",
    "        self.env = env\n",
    "        # Define the RL environment & network forward function\n",
    "        self.env_params = env_params\n",
    "        self.model_forward = model_forward\n",
    "\n",
    "        if num_env_steps is None:\n",
    "            self.num_env_steps = self.env_params.max_requests\n",
    "        else:\n",
    "            self.num_env_steps = num_env_steps\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def population_rollout(self, rng_eval, policy_params):\n",
    "        \"\"\"Reshape parameter vector and evaluate the generation.\"\"\"\n",
    "        # Evaluate population of nets on gymnax task - vmap over rng & params\n",
    "        pop_rollout = jax.vmap(self.batch_rollout, in_axes=(None, 0)) # Same rng different parmas\n",
    "        return pop_rollout(rng_eval, policy_params)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def batch_rollout(self, rng_eval, policy_params):\n",
    "        \"\"\"Evaluate a generation of networks on RL/Supervised/etc. task.\"\"\"\n",
    "        # vmap over different MC fitness evaluations for single network\n",
    "        batch_rollout = jax.vmap(self.single_rollout, in_axes=(0, None))\n",
    "        return batch_rollout(rng_eval, policy_params)\n",
    "\n",
    "    def pmap_batch_rollout(self, rng_eval, policy_params, device_count, batch_size):\n",
    "        \"\"\"Evaluate a generation of networks on RL/Supervised/etc. task.\"\"\"\n",
    "        # vmap over different MC fitness evaluations for single network\n",
    "        # Broadcast the params to each device and env (identical for each)\n",
    "        broadcast = lambda x: jnp.broadcast_to(x, (device_count, batch_size) + x.shape)\n",
    "        params = jax.tree_map(broadcast, policy_params)  # broadcast to cores and batch.\n",
    "        # Reshape the rngs so that each env on each device has a unique rng\n",
    "        reshape = lambda x: x.reshape((device_count, batch_size) + x.shape[1:])\n",
    "        pmap_rngs = reshape(jnp.stack(rng_eval))  # add dimension to pmap over.\n",
    "        # In-axes not specified so that everything is split (in_axes=None) instead of broadcast (in_axes=0)\n",
    "        batch_rollout = jax.vmap(self.single_rollout, axis_name=\"envs\")\n",
    "        batch_rollout = jax.pmap(batch_rollout, axis_name=\"devices\")\n",
    "        return batch_rollout(pmap_rngs, params)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def single_rollout(self, rng_input, policy_params):\n",
    "        \"\"\"Rollout a pendulum episode with lax.scan.\"\"\"\n",
    "        # Reset the environment\n",
    "        rng_reset, rng_episode = jax.random.split(rng_input)\n",
    "        obs, state = self.env.reset(rng_reset, self.env_params)\n",
    "\n",
    "        def policy_step(state_input, tmp):\n",
    "            \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "            obs, state, policy_params, rng, cum_reward, valid_mask = state_input\n",
    "            rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "            if self.model_forward is not None:\n",
    "                action = jnp.squeeze(self.model_forward(policy_params, obs, rng_net))\n",
    "            else:\n",
    "                action = self.env.action_space(self.env_params).sample(rng_net)\n",
    "            next_obs, next_state, reward, done, _ = self.env.step(\n",
    "                rng_step, state, action, self.env_params\n",
    "            )\n",
    "            new_cum_reward = cum_reward + reward * valid_mask\n",
    "            new_valid_mask = valid_mask * (1 - done)\n",
    "            carry = [\n",
    "                next_obs,\n",
    "                next_state,\n",
    "                policy_params,\n",
    "                rng,\n",
    "                new_cum_reward,\n",
    "                new_valid_mask,\n",
    "            ]\n",
    "            y = [obs, action, reward, next_obs, done]\n",
    "            return carry, y\n",
    "\n",
    "        # Scan over episode step loop\n",
    "        carry_out, scan_out = jax.lax.scan(\n",
    "            policy_step,\n",
    "            [\n",
    "                obs,\n",
    "                state,\n",
    "                policy_params,\n",
    "                rng_episode,\n",
    "                jnp.array([0.0]),\n",
    "                jnp.array([1.0]),\n",
    "            ],\n",
    "            (),\n",
    "            self.num_env_steps,\n",
    "        )\n",
    "        # Return the sum of rewards accumulated by agent in episode rollout\n",
    "        obs, action, reward, next_obs, done = scan_out\n",
    "        cum_return = carry_out[-2]\n",
    "        return obs, action, reward, next_obs, done, cum_return\n",
    "\n",
    "    @property\n",
    "    def input_shape(self):\n",
    "        \"\"\"Get the shape of the observation.\"\"\"\n",
    "        rng = jax.random.PRNGKey(0)\n",
    "        obs, state = self.env.reset(rng, self.env_params)\n",
    "        return obs.shape\n",
    "\n",
    "class TimeIt():\n",
    "    def __init__(self, tag, frames=None):\n",
    "        self.tag = tag\n",
    "        self.frames = frames\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = timeit.default_timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed_secs = timeit.default_timer() - self.start\n",
    "        msg = self.tag + (': Elapsed time=%.2fs' % self.elapsed_secs)\n",
    "        if self.frames:\n",
    "            msg += ', FPS=%.2e' % (self.frames / self.elapsed_secs)\n",
    "        print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define rollout manager for env\n",
    "manager = RolloutWrapper(model.apply, env=env, env_params=env_params)\n",
    "\n",
    "# Simple single episode rollout for policy\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    manager.single_rollout(rng, policy_params)  # compiles\n",
    "\n",
    "#num_frames = cores_count * iterations * rollout_len * batch_size\n",
    "with TimeIt(tag='EXECUTION', frames=env_params.max_requests):\n",
    "    # Run compiled func\n",
    "    obs, action, reward, next_obs, done, cum_ret = manager.single_rollout(rng, policy_params)\n",
    "    reward.block_until_ready()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs.shape, action.shape, reward.shape, next_obs.shape, done.shape, cum_ret.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# What about pmapped rollouts?\n",
    "# Create a random key for every env on every device\n",
    "num_envs_per_device = 10\n",
    "rng_envs = jax.random.split(rng, num_envs_per_device*num_devices)\n",
    "\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    manager.pmap_batch_rollout(rng_envs, policy_params, num_devices, num_envs_per_device)  # compiles\n",
    "\n",
    "with TimeIt(tag='EXECUTION', frames=env_params.max_requests*num_envs_per_device*num_devices):\n",
    "    obs, action, reward, next_obs, done, cum_ret = manager.pmap_batch_rollout(\n",
    "        rng_envs, policy_params, num_devices, num_envs_per_device\n",
    "    )\n",
    "    reward.block_until_ready()\n",
    "#obs.shape, action.shape, reward.shape, next_obs.shape, done.shape, cum_ret.shape\n",
    "obs[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#print(jax.lax.pmean(jax.lax.pmean(cum_ret, axis_name=\"envs\"), axis_name=\"devices\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mpopg3jUMbU1"
   },
   "outputs": [],
   "source": [
    "# Multiple rollouts for same network (different rng, e.g. eval)\n",
    "num_envs = 10\n",
    "rng_batch = jax.random.split(rng, num_envs)\n",
    "\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    manager.batch_rollout(rng_batch, policy_params)  # compiles\n",
    "\n",
    "with TimeIt(tag='EXECUTION', frames=env_params.max_requests*num_envs):\n",
    "    obs, action, reward, next_obs, done, cum_ret = manager.batch_rollout(\n",
    "        rng_batch, policy_params\n",
    "    )\n",
    "    reward.block_until_ready()\n",
    "obs.shape, action.shape, reward.shape, next_obs.shape, done.shape, cum_ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multiple rollouts for different networks + rng (e.g. for ES)\n",
    "batch_params = jax.tree_map(  # Stack parameters or use different\n",
    "    lambda x: jnp.tile(x, (2, 1)).reshape(2, *x.shape), policy_params\n",
    ")\n",
    "\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    manager.population_rollout(rng_batch, batch_params)  # compiles\n",
    "\n",
    "with TimeIt(tag='EXECUTION', frames=env_params.max_requests*num_envs):\n",
    "    obs, action, reward, next_obs, done, cum_ret = manager.population_rollout(\n",
    "        rng_batch, batch_params\n",
    "    )\n",
    "    reward.block_until_ready()\n",
    "obs.shape, action.shape, reward.shape, next_obs.shape, done.shape, cum_ret.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qFn3NPmMbU2"
   },
   "source": [
    "## Visualizing Episode Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAYP-EMjMbU3"
   },
   "outputs": [],
   "source": [
    "from gymnax.visualize import Visualizer\n",
    "\n",
    "state_seq, reward_seq = [], []\n",
    "rng, rng_reset = jax.random.split(rng)\n",
    "obs, env_state = env.reset(rng_reset, env_params)\n",
    "t_counter = 0\n",
    "while True:\n",
    "    state_seq.append(env_state)\n",
    "    rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "    action = env.action_space(env_params).sample(rng_act)\n",
    "    next_obs, next_env_state, reward, done, info = env.step(\n",
    "        rng_step, env_state, action, env_params\n",
    "    )\n",
    "    reward_seq.append(reward)\n",
    "    t_counter += 1\n",
    "    if done or t_counter >= 50:\n",
    "        break\n",
    "    else:\n",
    "        obs = next_obs\n",
    "        env_state = next_env_state\n",
    "\n",
    "cum_rewards = jnp.cumsum(jnp.array(reward_seq))\n",
    "vis = Visualizer(env, env_params, state_seq, cum_rewards)\n",
    "vis.animate(f\"anim.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5DMQXNrMbU4",
    "outputId": "ad15f6b1-979b-4f45-d933-3497782f9cc3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='anim.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Benchmarking against CPU VoneEnv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from heuristics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import env.envs\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from util_funcs import make_env, parse_args, choose_schedule\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "num_envs = 50\n",
    "num_steps = 25\n",
    "\n",
    "args = parse_args()\n",
    "conf = yaml.safe_load(Path(\"./config/agent_conus.yaml\").read_text())\n",
    "#print(args)\n",
    "#print(conf)\n",
    "\n",
    "env = [\n",
    "    make_env(conf[\"env_name\"], seed=i, **conf[\"env_args\"])\n",
    "    for i in range(num_envs)\n",
    "]\n",
    "#env = DummyVecEnv(env)\n",
    "env = SubprocVecEnv(env, start_method=\"fork\")\n",
    "\n",
    "agent_kwargs = dict(\n",
    "    verbose=0,\n",
    "    device=\"cuda\",\n",
    "    gamma=args.gamma,\n",
    "    learning_rate=choose_schedule(args.lr_schedule, args.learning_rate),\n",
    "    gae_lambda=args.gae_lambda,\n",
    "    n_steps=args.n_steps,\n",
    "    batch_size=10,\n",
    "    clip_range=choose_schedule(args.clip_range_schedule, args.clip_range),\n",
    "    clip_range_vf=choose_schedule(args.clip_range_vf_schedule, args.clip_range_vf),\n",
    "    n_epochs=args.n_epochs,\n",
    "    ent_coef=args.ent_coef,\n",
    "    policy_kwargs={\"net_arch\": dict(pi=[64, 64], vf=[64, 64])},\n",
    ")\n",
    "if args.multistep_masking:\n",
    "    agent_kwargs.update(\n",
    "        multistep_masking=args.multistep_masking,\n",
    "        multistep_masking_attr=args.multistep_masking_attr,\n",
    "        multistep_masking_n_steps=args.multistep_masking_n_steps,\n",
    "        action_interpreter=args.action_interpreter,\n",
    "    )\n",
    "agent_args = (\"MultiInputPolicy\", env)\n",
    "\n",
    "model = PPO(*agent_args, **agent_kwargs)\n",
    "\n",
    "action = np.array([[0,0,0,0,0,0]]*num_envs)\n",
    "action = np.tile(env.action_space.sample(), (num_envs, 1))\n",
    "#print(action)\n",
    "#print(action.shape)\n",
    "\n",
    "#logger = logging.getLogger(__name__)\n",
    "#logger.setLevel(logging.WARN)\n",
    "\n",
    "print('start')\n",
    "with TimeIt(tag='EXECUTION', frames=num_envs*num_steps):\n",
    "    for i in range(num_steps):\n",
    "        action = env.action_space.sample()\n",
    "        action = np.tile(env.action_space.sample(), (num_envs, 1))\n",
    "        x = env.step(action)\n",
    "        # eva = evaluate_policy(\n",
    "        #     model,\n",
    "        #     env,\n",
    "        #     n_eval_episodes=1,\n",
    "        # )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
