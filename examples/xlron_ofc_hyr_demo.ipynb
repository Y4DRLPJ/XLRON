{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4fdbe9acfaf744",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# OFC 2024 Hack Your Research\n",
    "\n",
    "<img src=\"../docs/images/xlron_logo_upscaled.png\"  width=\"600\">\n",
    "\n",
    "# for &#x1f525;_blazingly fast_&#x1f525; RL on optical networks problems! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea049838d214b28",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### What are we going to do today?\n",
    "#### 0. Learn how XLRON can help accelerate your research\n",
    "#### 1. Setup a virtual environment to start experiencing &#x1f3ce; _the speed_ &#x1f3ce;\n",
    "#### 2. JAX and functional programming fundamentals &#x2699;\n",
    "#### 3. Let's talk about data structures &#x1f916;\n",
    "#### 4. A simple reinforcement learning loop &#x267b;\n",
    "#### 5. BRING ON THE GPU POWER &#x1f4a5;\n",
    "#### 6. Evaluation and plotting &#x1f4c8;\n",
    "#### BONUS: Experiment tracking with Weights and Biases &#x1f4d6;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036e77295be2b63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## FIRST THINGS FIRST\n",
    "### What's an XLRON?\n",
    "XLRON (“ex-el-er-on”) (“**X**”-elerated **L**earning and **R**esource Allocation for **O**ptical **N**etworks) is a Python library for fast and scalable reinforcement learning on optical networks. It is built on top of JAX, a library for composable function transformations and efficient GPU/TPU computing. XLRON is designed to be easy to use, and to train on lots of parallel environments to achieve high throughput. It is also designed to be easy to extend, so that researchers can experiment with new algorithms and architectures.\n",
    "\n",
    "### What good is it?\n",
    "XLRON gives a massive boost to the speed at which you can train reinforcement learning agents on optical networks problems. This allows rapid experimentation, \n",
    "\n",
    "### How does it do it?\n",
    "- Massive parallelization on accelerator hardware\n",
    "- Speedup from JIT compilation\n",
    "- Avoid CPU-GPU data transfer bottleneck\n",
    "- Avoid Python interpreter overhead\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../docs/images/xlron_diagram.png\"  width=\"1200\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092bb1a0fa96385",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. Let's install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067705f87ab7d96",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set up virtual environment\n",
    "!pip install virtualenv\n",
    "!virtualenv venv\n",
    "!source venv/bin/activate\n",
    "# install required packages\n",
    "!pip install -r https://raw.githubusercontent.com/micdoh/XLRON/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17436fe638b912f5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## N.B. if you're running this notebook on a GPU machine, you can install the GPU version of JAX by running the following command (or substitute cuda12 for cuda11 depending on your CUDA version):\n",
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef3678c7ce7993",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "# 2. JAX and functional programming\n",
    "\n",
    "### What's JAX?\n",
    "JAX is a library for numerical computing that lets you write high-performance, vectorized code that runs on GPU, TPU, and CPU. It is a composable function transformation library designed for high performance numerical computing and large-scale machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2966ef81c44b474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T09:44:41.172171Z",
     "start_time": "2024-11-04T09:44:40.630256Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Let's import jax and see what devices we have access to\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4abb7477c100b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "\n",
    "### What's functional programming?\n",
    "Functional programming requires all functions to be pure (i.e. have no \"side effects\"; they give the same output for the same input every time), and a program is expressed as a composition of these transformation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8b763b85dbc378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:25.825407Z",
     "start_time": "2024-03-24T22:57:25.590418Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def impure_print_side_effect(x):\n",
    "  print(\"Executing function\")  # This is a side-effect\n",
    "  return x\n",
    "\n",
    "# The side-effects appear during the first run\n",
    "print(\"First call: \", jax.jit(impure_print_side_effect)(4.))\n",
    "\n",
    "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
    "# This is because JAX now invokes a cached compilation of the function\n",
    "print(\"Second call: \", jax.jit(impure_print_side_effect)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "print(\"Third call, different type: \", jax.jit(impure_print_side_effect)(jnp.array([5.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83237323d293f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Explaining all the intricacies of JAX and functional programming is beyond the scope of this notebook, but you can find more information in the [JAX documentation](https://jax.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0193ce3ed24076",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "# 3. Data structures in XLRON - state and parameters\n",
    "\n",
    "Now that we know a bit about JAX, functional programming, and the constraints it imposes, we can talk about how XLRON structures its data. Unlike other RL environment libraries (e.g. [optical-rl-gym](https://github.com/carlosnatalino/optical-rl-gym)) that store the environment state as attributes of the environment class object, the environment classes defined in XLRON simply provide a namespace to store the environment methods such as step, reset. The actual data for the environment state and parameters are stored in separate dataclass objects, that must be passed to the environment methods as arguments.\n",
    "\n",
    "### Why are the state and parameters stored separately?\n",
    "The environment state is mutable - we expect it to change every time it is passed to the `step()` method with an action. The environment parameters, on the other hand, are constant throughout the lifetime of the environment. Separating the 'traced' variables of the state from the 'static' variables of the environment is useful due to the internals of JAX and JIT compilation - by marking the parameters as `static_args` when passing them to a function, JAX can optimize the function by tracing only the state variables, which saves a ton of memory for our compiled program and allows our params to be used as static lookup values or tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4f8c926fed7c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:36.654465Z",
     "start_time": "2024-03-24T22:57:26.610428Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from xlron.environments.rsa import make_rsa_env\n",
    "\n",
    "# JAX handles pseudo-random number generation using PRNGKeys\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_init, key_reset, key_policy, key_step = jax.random.split(rng, 5)\n",
    "\n",
    "config_dict = {\n",
    "    \"topology_name\": \"nsfnet\",\n",
    "    \"link_resources\": 50,\n",
    "    \"max_timesteps\": 1e3,\n",
    "}\n",
    "env, env_params = make_rsa_env(config_dict)\n",
    "\n",
    "# Inspect default environment settings\n",
    "env_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627380458161d4ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:37.198446Z",
     "start_time": "2024-03-24T22:57:36.656460Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "obs, state = env.reset(key_reset, env_params)\n",
    "obs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83cebeacebbdba2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:38.202603Z",
     "start_time": "2024-03-24T22:57:37.200468Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "action = env.action_space(env_params).sample(key_policy)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "action, n_obs, n_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fdd86dc581773",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "# 4. A simple reinforcement learning loop\n",
    "\n",
    "<img src=\"../docs/images/rl_loop.png\"  width=\"1200\">\n",
    "\n",
    "Let's bring together the environment state, parameters, an agent, and the environment methods to build a reinforcement learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f3db62a575cc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:40.249960Z",
     "start_time": "2024-03-24T22:57:38.205026Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# First we need to define an agent\n",
    "from xlron.models import ActorCriticMLP\n",
    "rng, key_net, key_action, key_step = jax.random.split(rng, 4)\n",
    "\n",
    "network = ActorCriticMLP(\n",
    "    [env.action_space(env_params).n],\n",
    "    activation=\"tanh\",\n",
    "    num_layers=2,\n",
    "    num_units=64\n",
    ")\n",
    "\n",
    "init_input_layer = tuple([jnp.zeros(env.observation_space(env_params).n)])\n",
    "network_params = network.init(key_net, *init_input_layer)\n",
    "\n",
    "network, len(*init_input_layer), network_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db4b7ca0a5f644c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:40.831787Z",
     "start_time": "2024-03-24T22:57:40.252365Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We get an action by doing a forward pass through the network, with the (flattened) observation as input\n",
    "pi, value = network.apply(network_params, obs)\n",
    "\n",
    "# pi is a probability distribution over the action space. Let's sample from it\n",
    "action = pi[0].sample(seed=key_action)\n",
    "\n",
    "# Now let's step the environment with this action\n",
    "obs, state, reward, done, info = env.step(key_step, state, action, env_params)\n",
    "\n",
    "obs, state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e01cc659a71059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:39:37.948025Z",
     "start_time": "2024-03-24T22:35:55.127011Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's reset the environment, run for 1000 steps and see what the cumulative reward looks like in a plot\n",
    "obs, state = env.reset(key_reset, env_params)\n",
    "# N.B. the reward is defined as +1 success -1 failure by default\n",
    "cumulative_reward = 0\n",
    "cumulative_rewards = []\n",
    "for _ in range(10000):\n",
    "    rng, key_action, key_step = jax.random.split(rng, 3)\n",
    "    pi, value = network.apply(network_params, obs)\n",
    "    action = pi[0].sample(seed=key_action)\n",
    "    obs, state, reward, done, info = env.step(key_step, state, action, env_params)\n",
    "    cumulative_reward += reward\n",
    "    cumulative_rewards.append(cumulative_reward)\n",
    "    if done:\n",
    "        obs, state = env.reset(key_reset, env_params)\n",
    "        cumulative_reward = 0\n",
    "\n",
    "cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4946cc538072ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:39:38.558945Z",
     "start_time": "2024-03-24T22:39:37.949861Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Cumulative reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccd347d5860ef8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Hmmm, doesn't look great! But we haven't added a learning step yet to update the network parameters. Let's do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ddf8072d22694",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### What about adding in the training loop?\n",
    "XLRON has a super fast JAX implementation of the PPO RL algorithm (adapted from  [PureJaxRL](https://github.com/luchris429/purejaxrl)), which is easily accessed through the convenient `make_train()` function.\n",
    "\n",
    "This will take care of the training loop, including generating parallel environments, running the agent, and updating the network parameters. The training loop is JIT-compiled to run on the CPU or GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58735d7223ce225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:57:40.841333Z",
     "start_time": "2024-03-24T22:57:40.833870Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's make a training loop, JIT-compile it to the CPU and run it!\n",
    "from xlron.environments.wrappers import TimeIt\n",
    "from xlron.train.ppo import make_train\n",
    "import sys\n",
    "from absl import flags\n",
    "import xlron.train.parameter_flags\n",
    "# FLAGS are commandline arguments that can be passed to the training loop\n",
    "# Let's alter them in-code here\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS((\"--env_type\", \"rmsa\", # Type of environment to train on\",\n",
    "      \"--k\", \"5\",  # \"Number of paths to consider for each request\",\n",
    "      \"--topology_name\",   \"nsfnet\", #\"Name of the network topology to use\",\n",
    "      \"--link_resources\", \"50\",  # \"Number of resources per link\",\n",
    "      \"--max_requests\", \"1e3\",  # \"Maximum number of requests to generate\",\n",
    "      \"--max_timesteps\", \"1e3\",  # \"Maximum number of timesteps per episode\",\n",
    "      \"--TOTAL_TIMESTEPS\", \"100000\",  # \"Total number of timesteps to train for\",\n",
    "      \"--incremental_loading\",  # \"Whether to use incremental loading (non-expiring requests)\",\n",
    "      \"--NUM_STEPS\", \"50\",  # \"Number of steps per epoch\",\n",
    "      \"--NUM_ENVS\", \"10\",  # \"Number of parallel environments to run\",\n",
    "      \"--NUM_SEEDS\", \"1\",  # \"Number of seeds to run\",\n",
    "      \"--UPDATE_EPOCHS\", \"10\",  # \"Number of epochs to train for\",\n",
    "      \"--LR\", \"5e-5\",  # \"Learning rate\",\n",
    "      \"--LR_SCHEDULE\", \"linear\",  # \"Learning rate schedule\",\n",
    "      \"--VISIBLE_DEVICES\", \"0\",  # \"Visible devices\",\n",
    "       \"--ACTION_MASKING\",  # \"Whether to use action masking\",\n",
    "       \"--NUM_LAYERS\", \"2\",  # \"Number of layers in the network\",\n",
    "       \"--NUM_UNITS\", \"128\",  # \"Number of units per layer\",\n",
    "      ))\n",
    "# More options are available - check out the docs for more info https://micdoh.github.io/XLRON/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c4f4e3b64b0cb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:58:33.586184Z",
     "start_time": "2024-03-24T22:57:40.843297Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# NUM_ENVS determines how many parallel environments to run (with the same agent parameters for each)\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    train_jit = jax.jit(make_train(FLAGS), backend='cpu').lower(rng).compile()\n",
    "    \n",
    "print(f\"Running {FLAGS.TOTAL_TIMESTEPS * FLAGS.NUM_SEEDS} timesteps on CPU\")\n",
    "with TimeIt(tag='EXECUTION', frames=FLAGS.TOTAL_TIMESTEPS * FLAGS.NUM_SEEDS):\n",
    "    out_cpu = train_jit(rng)\n",
    "    out_cpu[\"metrics\"][\"episode_returns\"].block_until_ready()  # Wait for all devices to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325efd954c8fc9b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:02:44.860660Z",
     "start_time": "2024-03-24T22:58:33.589752Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also parallelise across different random seeds (with different agent parameters for each)\n",
    "FLAGS.NUM_ENVS = 1\n",
    "FLAGS.NUM_SEEDS = 5\n",
    "FLAGS.TOTAL_TIMESTEPS = 50_000\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    rng_seeds = jax.random.split(rng, FLAGS.NUM_SEEDS)\n",
    "    train_jit = jax.jit(jax.vmap(make_train(FLAGS)), backend='cpu').lower(rng_seeds).compile()\n",
    "    \n",
    "print(f\"Running {FLAGS.TOTAL_TIMESTEPS * FLAGS.NUM_SEEDS} timesteps on CPU\")\n",
    "with TimeIt(tag='EXECUTION', frames=FLAGS.TOTAL_TIMESTEPS * FLAGS.NUM_SEEDS):\n",
    "    out_cpu_seeds = train_jit(rng_seeds)\n",
    "    out_cpu_seeds[\"metrics\"][\"episode_returns\"].block_until_ready()  # Wait for all devices to finish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b3eac85043639",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "# 5. Using the GPU &#x1f4a5;\n",
    "\n",
    "If connected to a GPU-enabled machine, let's run the same training loop with 3000 parallel environments and see how long it takes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "401f1524c3cc1e77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:07:00.296714Z",
     "start_time": "2024-03-24T23:04:59.116276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "FLAGS.NUM_SEEDS = 1\n",
    "FLAGS.NUM_ENVS = 3000\n",
    "FLAGS.TOTAL_TIMESTEPS = 30_000_000\n",
    "\n",
    "with TimeIt(tag='COMPILATION'):\n",
    "    train_jit = jax.jit(make_train(FLAGS), backend='gpu').lower(rng).compile()\n",
    "    \n",
    "print(f\"Running {FLAGS.TOTAL_TIMESTEPS * FLAGS.NUM_SEEDS} timesteps on GPU\")\n",
    "with TimeIt(tag='EXECUTION', frames=FLAGS.TOTAL_TIMESTEPS * FLAGS.NUM_SEEDS):\n",
    "    out_gpu = train_jit(rng)\n",
    "    out_gpu[\"metrics\"][\"episode_returns\"].block_until_ready()  # Wait for all devices to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba274101d911a0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "# 6. Evaluation and plotting &#x1f4c8;\n",
    "\n",
    "\n",
    "### N.B. we haven't done any hyperparameter tuning so we shouldn't expect great results here! \n",
    "\n",
    "#### The main thing is the training time &#x23F0;\n",
    "\n",
    "See the bonus section on wandb and hyperparameter sweeps to learn how to get the best hyperparameters to make your agents sing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46d5d4f465021c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:12:45.381007Z",
     "start_time": "2024-03-24T23:12:45.213874Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the results of the training loop\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Let's use a moving average to make the plots easier to read\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "cum_returns_mean_cpu = out_cpu[\"metrics\"][\"cum_returns\"].mean(-1).reshape(-1)\n",
    "cum_returns_mean_cpu_smooth = moving_average(cum_returns_mean_cpu, 50)\n",
    "plt.plot(cum_returns_mean_cpu_smooth, label=\"CPU\")\n",
    "plt.title(\"Cumulative returns CPU\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Cumulative return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "818faced78da8bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:13:12.011351Z",
     "start_time": "2024-03-24T23:13:11.701937Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the results for our multiple seed run on CPU\n",
    "for i in range(5):\n",
    "    cum_returns_mean_cpu_seeds = out_cpu_seeds[\"metrics\"][\"cum_returns\"][i].mean(-1).reshape(-1)\n",
    "    cum_returns_mean_cpu_seeds_smooth = moving_average(cum_returns_mean_cpu_seeds, 50)\n",
    "    plt.plot(cum_returns_mean_cpu_seeds_smooth, label=f\"Seed {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72f490f7024bc9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:09:57.072366Z",
     "start_time": "2024-03-24T23:09:56.819392Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cum_returns_mean_gpu = out_gpu[\"metrics\"][\"cum_returns\"].mean(-1).reshape(-1)\n",
    "plt.plot(cum_returns_mean_gpu, label=\"GPU\")\n",
    "# The plot looks smoother because we're taking the mean of 3000 parallel environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bcf555be03fa714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:22:35.349057Z",
     "start_time": "2024-03-24T23:22:35.168513Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's rebase so that the x-axis is time and plot CPU and GPU results\n",
    "# multiply by total time, divide by total number of steps\n",
    "cpu_time = np.arange(len(cum_returns_mean_cpu)) * 38.36 / 1e5\n",
    "gpu_time = np.arange(len(cum_returns_mean_gpu)) * 110.04 / 3e7\n",
    "# let's smooth the CPU results out so we can see the plots better\n",
    "cum_returns_mean_cpu_smooth = moving_average(cum_returns_mean_cpu, 10)\n",
    "\n",
    "plt.plot(cpu_time[:len(cum_returns_mean_cpu_smooth)], cum_returns_mean_cpu_smooth, label=\"CPU\")\n",
    "plt.plot(gpu_time, cum_returns_mean_gpu, label=\"GPU\")\n",
    "plt.title(\"Cumulative returns CPU vs GPU\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Cumulative return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de4dc0f3dc0200",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Don't forget to check out the documentation site if you want to learn more: [https://micdoh.github.io/XLRON/](https://micdoh.github.io/XLRON/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c493f9e6ec8c928",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "# BONUS: Experiment tracking with Weights and Biases &#x1f4d6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "741ca9f1b2ec8676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T23:28:43.236956Z",
     "start_time": "2024-03-24T23:25:41.942516Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# If you have a Weights and Biases account, you can log your experiments simply by adding the WANDB flag\n",
    "! python -m xlron.train.train --env_type rmsa --k 5 --topology_name nsfnet --link_resources 50 --max_requests 1e3 --max_timesteps 1e3 --TOTAL_TIMESTEPS 30e6 --incremental_loading --NUM_STEPS 50 --NUM_ENVS 3000 --NUM_SEEDS 1 --UPDATE_EPOCHS 10 --LR 5e-5 --LR_SCHEDULE linear --VISIBLE_DEVICES 0 --ACTION_MASKING --WANDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f17fbfdac65e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Wandb is also super useful for performing hyperparameter sweeps. See wandb docs for more details: [https://docs.wandb.ai/guides/sweeps](https://docs.wandb.ai/guides/sweeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d95cc4b69331f7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup a sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"cumulative_reward\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"incremental_loading\": {\"values\": [True, False]},\n",
    "        \"NUM_STEPS\": {\"values\": [50, 100, 200]},\n",
    "        \"NUM_ENVS\": {\"values\": [10, 100, 1000]},\n",
    "        \"NUM_SEEDS\": {\"values\": [1, 5, 10]},\n",
    "        \"UPDATE_EPOCHS\": {\"values\": [10, 20, 50]},\n",
    "        \"LR\": {\"values\": [1e-5, 5e-5, 1e-4]},\n",
    "        \"LR_SCHEDULE\": {\"values\": [\"linear\", \"cosine\", \"exponential\"]},\n",
    "        \"VISIBLE_DEVICES\": {\"values\": [0, 1, 2, 3]},\n",
    "        \"ACTION_MASKING\": {\"values\": [True]},\n",
    "    },\n",
    "}\n",
    "# initialise a sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"xlron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323957e095bf33c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# run an agent with the sweep_id\n",
    "!wandb agent $sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed115146dd6408",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
